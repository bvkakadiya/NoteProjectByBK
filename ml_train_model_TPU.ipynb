{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_train_model_TPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNWW4RvEOBLeNgYdBDPRqxg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bvkakadiya/NoteProjectByBK/blob/master/ml_train_model_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ORcORstXJUq",
        "colab_type": "code",
        "outputId": "828662a0-f01d-43b6-d8f3-6cccf8eeba8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "pip install tensorflow==1.13.1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
            "\u001b[K     |████████████████████████████████| 92.5MB 68kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 54.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.34.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.27.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 65.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (45.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o4qMhnkHT-n",
        "colab_type": "code",
        "outputId": "d06711f6-8e2c-4859-af58-1a55e8fb0be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "# To run and train model use below command \n",
        "# python3 train_model_ml.py  '-m' 'sl' '-e' '10' '-b' '256' '-alpha' '0.1' '-beta' '1.0' '-c' 'C5'\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import argparse\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from loss import symmetric_cross_entropy, cross_entropy\n",
        "from callback_util import LoggerCallback, SGDLearningRateTracker\n",
        "from util import get_lr_scheduler\n",
        "import distutils\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sKhTiJKUonN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Data_Sample_Path = r'A_train_samples.xlsx'\n",
        "X_data_loaded = pd.read_excel(Train_Data_Sample_Path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3AahKULUsE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Test_Data_label_Path = r'A_train_labels.xlsx'\n",
        "Y_data_loaded =pd.read_excel(Test_Data_label_Path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAMINQkvHhaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Input,Reshape, Conv1D, Dense, MaxPooling1D, Dropout, Flatten, Activation, BatchNormalization\n",
        "\n",
        "def get_model(seq_len=None, input_features=None, num_classes=10):\n",
        "    \n",
        "    inputs = Input(shape=(input_features, 1),dtype=tf.float32, name='input_layer')   \n",
        "    # x = inputs\n",
        "    # x = Reshape((input_features, 1))(inputs)\n",
        "    # Block 1\n",
        "    # inputs = Input(name='input', shape=(2000, 42))\n",
        "    x = Conv1D(64, 2, padding='same', kernel_initializer=\"he_normal\", name='block1_conv1')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv1D(128, 2, padding='same', kernel_initializer=\"he_normal\", name='block1_conv2')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = MaxPooling1D(2, strides=2, name='block1_pool')(x)\n",
        "\n",
        "    x = Flatten(name='flatten')(x)\n",
        " \n",
        "    x = Dense(256, kernel_initializer=\"he_normal\", kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), name='fc1')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu', name='lid')(x)\n",
        "\n",
        "    x = Dense(num_classes, kernel_initializer=\"he_normal\")(x)\n",
        "    x = Activation(tf.nn.softmax)(x)\n",
        "\n",
        "    # Create model.\n",
        "    model = Model(inputs, x)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cCrnxPaHQhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_input_fn(batch_size=1024):\n",
        "    # convert the inputs to a Dataset.\n",
        "    \n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32),y_train.astype(np.float32)))\n",
        "    # tf.dtypes.cast(dataset, tf.float32)\n",
        "    # shuffle, repeat, and batch the examples.\n",
        "    dataset = dataset.shuffle(1000).repeat().batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    # return the dataset.\n",
        "    return dataset\n",
        "\n",
        "def batch_generator(X_train,x_test,batch_size, sequence_length):\n",
        "    \"\"\"\n",
        "    Generator function for creating random batches of training-data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Infinite loop.\n",
        "    while True:\n",
        "        # Allocate a new array for the batch of input-signals.\n",
        "        x_shape = (batch_size, sequence_length, 42)\n",
        "        x_batch = np.zeros(shape=x_shape, dtype=np.float64)\n",
        "\n",
        "        # Allocate a new array for the batch of output-signals.\n",
        "        y_shape = (batch_size, sequence_length, 1)\n",
        "        y_batch = np.zeros(shape=y_shape, dtype=np.float64)\n",
        "        num_train = len(X_train)\n",
        "        # Fill the batch with random sequences of data.\n",
        "        for i in range(batch_size):\n",
        "            # Get a random start-index.\n",
        "            # This points somewhere into the training-data.\n",
        "            idx = np.random.randint(num_train - sequence_length)\n",
        "            \n",
        "            # Copy the sequences of data starting at this index.\n",
        "            x_batch[i] = X_train[idx:idx+sequence_length]\n",
        "            y_batch[i] = x_test[idx:idx+sequence_length]\n",
        "\n",
        "            yield (x_batch, y_batch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y67qyvhcZfK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 2000\n",
        "    # load data\n",
        "x =X_data_loaded.values\n",
        "y = pd.DataFrame(Y_data_loaded['B']).values\n",
        "X_train = MinMaxScaler().fit_transform(x)\n",
        "\n",
        "# x, w, y, z = train_test_split(x,y,test_size = 0.40,train_size =0.60)\n",
        "# X_train, x_validation, y_train, y_validation = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n",
        "y_train_clean = y\n",
        "x_validation=x \n",
        "y_validation=y\n",
        "y_train=y\n",
        "seq_len = X_train.shape[0]\n",
        "input_features = X_train.shape[1]\n",
        "X_train = np.array(X_train).reshape(seq_len, input_features, 1)\n",
        "num_classes = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEqiNoY0HKNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if distutils.version.LooseVersion(tf.__version__) < '1.14':\n",
        "#     raise Exception('This notebook is compatible with TensorFlow 1.14 or higher, for TensorFlow 1.13 or lower please use the previous version at https://github.com/tensorflow/tpu/blob/r1.13/tools/colab/shakespeare_with_tpu_and_keras.ipynb')\n",
        "\n",
        "# TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "# print(TPU_WORKER)\n",
        "\n",
        "def train(model_name, batch_size, epochs, asym, alpha, beta, column_name):\n",
        "    sequence_length = 2000\n",
        "    # load data\n",
        "    x =X_data_loaded.values\n",
        "    y = pd.DataFrame(Y_data_loaded[column_name]).values\n",
        "    X_train = MinMaxScaler().fit_transform(x)\n",
        "\n",
        "    # x, w, y, z = train_test_split(x,y,test_size = 0.40,train_size =0.60)\n",
        "    # X_train, x_validation, y_train, y_validation = train_test_split(x,y,test_size = 0.25,train_size =0.75)\n",
        "    y_train_clean = y\n",
        "    x_validation=x \n",
        "    y_validation=y\n",
        "    y_train=y\n",
        "    seq_len = X_train.shape[0]\n",
        "    input_features = X_train.shape[1]\n",
        "    num_classes = 10\n",
        "   \n",
        "    # X_train = np.array(X_train).reshape(seq_len, input_features, 1)\n",
        "    # x_validation = np.array(x_validation).reshape(x_validation.shape[0], input_features, 1)\n",
        "    generator = batch_generator(X_train,y_train, batch_size=batch_size,sequence_length=sequence_length)\n",
        "    x_batch, y_batch = next(generator)\n",
        "    print(x_batch.shape)\n",
        "    print(y_batch.shape)\n",
        "    print(\"seq_len\", seq_len, \"num_classes\", num_classes, \"input_features:\", input_features)\n",
        "    \n",
        "    # load model\n",
        "    # resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)\n",
        "    # tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "    # strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "    # with strategy.scope():\n",
        "    model = get_model(seq_len, input_features, num_classes=10)\n",
        "    model.summary()\n",
        "    \n",
        "    # Select Optimizer\n",
        "    optimizer = SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
        "\n",
        "    # create loss\n",
        "    loss = symmetric_cross_entropy(alpha,beta)\n",
        "    \n",
        "    # model\n",
        "    model.compile(\n",
        "        loss=loss,\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    if asym:\n",
        "        model_save_file = \"asym_%s.{epoch:02d}.hdf5\" % (model_name)\n",
        "    else:\n",
        "        model_save_file = \"model_%s_{epoch:02d}_%s.hdf5\" % (model_name, column_name)\n",
        "    \n",
        "    ## do real-time updates using callbakcs\n",
        "    callbacks = []\n",
        "\n",
        "    cp_callback = ModelCheckpoint(model_save_file,\n",
        "                                    monitor='val_loss',\n",
        "                                    verbose=0,\n",
        "                                    save_best_only=False,\n",
        "                                    save_weights_only=True,\n",
        "                                    period=1)\n",
        "    callbacks.append(cp_callback)\n",
        "    \n",
        "    # learning rate scheduler if use sgd\n",
        "    lr_scheduler = get_lr_scheduler()\n",
        "    callbacks.append(lr_scheduler)\n",
        "    callbacks.append(SGDLearningRateTracker(model))\n",
        "    \n",
        "    # acc, loss, lid\n",
        "    log_callback = LoggerCallback(model, X_train, y_train, y_train_clean, x_validation, y_validation, model_name, asym, epochs, alpha, beta)\n",
        "    callbacks.append(log_callback)\n",
        "    generator = batch_generator(X_train,y_train, batch_size=batch_size,sequence_length=sequence_length)\n",
        "    # train model\n",
        "    # model.fit(generator=generator,\n",
        "    #             steps_per_epoch = int(len(X_train) / batch_size), \n",
        "    #             epochs=epochs, \n",
        "    #             validation_data = (x_validation, y_validation), \n",
        "    #             validation_steps = int(len(x_validation) / batch_size),\n",
        "    #             verbose=1, \n",
        "    #             callbacks=callbacks)\n",
        "    try:\n",
        "        device_name = os.environ['COLAB_TPU_ADDR']\n",
        "        TPU_ADDRESS = 'grpc://' + device_name\n",
        "        print('Found TPU at: {}'.format(TPU_ADDRESS))\n",
        "    except KeyError:\n",
        "        print('TPU not found')\n",
        "  \n",
        "    tpu_model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "        model,\n",
        "        strategy=tf.contrib.tpu.TPUDistributionStrategy(\n",
        "            tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS))\n",
        "    )\n",
        "\n",
        "    tpu_model.fit(train_input_fn,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch=int(len(X_train) / batch_size),\n",
        "                    #validation_data=validation_data,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu1fjrLLHxKV",
        "colab_type": "code",
        "outputId": "6fd21c91-a751-47b0-a2f6-950770dec6c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(model_name ='sl', batch_size =256, epochs =7, asym =False, alpha =0.1, beta =0.1, column_name = 'B')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 2000, 42)\n",
            "(256, 2000, 1)\n",
            "seq_len 175341 num_classes 10 input_features: 42\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (InputLayer)     (None, 42, 1)             0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv1D)        (None, 42, 64)            192       \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_9 (Ba (None, 42, 64)            256       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 42, 64)            0         \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv1D)        (None, 42, 128)           16512     \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_10 (B (None, 42, 128)           512       \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 42, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 42, 128)           0         \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling1D)   (None, 21, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2688)              0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 256)               688384    \n",
            "_________________________________________________________________\n",
            "batch_normalization_v1_11 (B (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "lid (Activation)             (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 709,450\n",
            "Trainable params: 708,554\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Found TPU at: grpc://10.16.45.138:8470\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.16.45.138:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 532481688932830823)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 9075978980676846509)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2271030795561816754)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14985408474852916931)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 792939484355836003)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 1508397990560803216)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 12562080377216199761)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11264689030905832280)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 7108042947840315498)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 2589602644185795617)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 4860874614823781974)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.10000000149011612, 'momentum': 0.8999999761581421, 'decay': 9.999999747378752e-05, 'nesterov': False}\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.10000000149011612, 'momentum': 0.8999999761581421, 'decay': 9.999999747378752e-05, 'nesterov': False}\n",
            "init lr: 0.1000, current lr: 0.1000, decay: 0.0001, iterations: 0.0\n",
            "Epoch 1/10\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(1024,), dtype=tf.int32, name=None), TensorSpec(shape=(1024, 42, 1), dtype=tf.float32, name=None), TensorSpec(shape=(1024, 1), dtype=tf.float32, name=None)]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning SGD {'lr': 0.10000000149011612, 'momentum': 0.8999999761581421, 'decay': 9.999999747378752e-05, 'nesterov': False}\n",
            "INFO:tensorflow:Remapping placeholder for input_layer\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.SGD object at 0x7fde433c8908> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 8.50048041343689 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.10000000149011612 {0.1}\n",
            "INFO:tensorflow:CPU -> TPU momentum: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU decay: 9.999999747378752e-05 {1e-04}\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "683/684 [============================>.] - ETA: 0s - loss: 14.2268 - acc: 0.0384INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None]\n",
            "684/684 [==============================] - 51s 74ms/step - loss: 14.2225 - acc: 0.0384\n",
            "init lr: 0.1000, current lr: 0.0936, decay: 0.0001, iterations: 684.0\n",
            "Epoch 2/10\n",
            "681/684 [============================>.] - ETA: 0s - loss: 13.7188 - acc: 0.0088INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None]\n",
            "684/684 [==============================] - 24s 36ms/step - loss: 13.7064 - acc: 0.0088\n",
            "init lr: 0.1000, current lr: 0.0880, decay: 0.0001, iterations: 1368.0\n",
            "Epoch 3/10\n",
            "681/684 [============================>.] - ETA: 0s - loss: 13.7178 - acc: 0.0059INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None]\n",
            "684/684 [==============================] - 24s 35ms/step - loss: 13.7067 - acc: 0.0058\n",
            "init lr: 0.1000, current lr: 0.0830, decay: 0.0001, iterations: 2052.0\n",
            "Epoch 4/10\n",
            "682/684 [============================>.] - ETA: 0s - loss: 13.7144 - acc: 0.0117INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None]\n",
            "684/684 [==============================] - 23s 34ms/step - loss: 13.7066 - acc: 0.0117\n",
            "init lr: 0.1000, current lr: 0.0785, decay: 0.0001, iterations: 2736.0\n",
            "Epoch 5/10\n",
            "681/684 [============================>.] - ETA: 0s - loss: 13.7042 - acc: 0.0059INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None]\n",
            "684/684 [==============================] - 24s 35ms/step - loss: 13.7075 - acc: 0.0058\n",
            "init lr: 0.1000, current lr: 0.0745, decay: 0.0001, iterations: 3420.0\n",
            "Epoch 6/10\n",
            "682/684 [============================>.] - ETA: 0s - loss: 13.7001 - acc: 0.0073INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None, None]\n",
            "684/684 [==============================] - 24s 35ms/step - loss: 13.7069 - acc: 0.0073\n",
            "init lr: 0.1000, current lr: 0.0709, decay: 0.0001, iterations: 4104.0\n",
            "Epoch 7/10\n",
            "682/684 [============================>.] - ETA: 0s - loss: 13.6931 - acc: 0.0088INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None, None, None]\n",
            "684/684 [==============================] - 24s 35ms/step - loss: 13.7002 - acc: 0.0088\n",
            "init lr: 0.1000, current lr: 0.0676, decay: 0.0001, iterations: 4788.0\n",
            "Epoch 8/10\n",
            "682/684 [============================>.] - ETA: 0s - loss: 13.6928 - acc: 0.0000e+00INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None, None, None, None]\n",
            "684/684 [==============================] - 24s 35ms/step - loss: 13.6999 - acc: 0.0000e+00\n",
            "init lr: 0.1000, current lr: 0.0646, decay: 0.0001, iterations: 5472.0\n",
            "Epoch 9/10\n",
            "681/684 [============================>.] - ETA: 0s - loss: 13.6893 - acc: 0.0103INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None, None, None, None, None]\n",
            "684/684 [==============================] - 25s 36ms/step - loss: 13.6999 - acc: 0.0102\n",
            "init lr: 0.1000, current lr: 0.0619, decay: 0.0001, iterations: 6156.0\n",
            "Epoch 10/10\n",
            "681/684 [============================>.] - ETA: 0s - loss: 13.6893 - acc: 0.0147INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.10000000149011612\n",
            "INFO:tensorflow:TPU -> CPU momentum: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU decay: 9.999999747378752e-05\n",
            "INFO:tensorflow:TPU -> CPU nesterov: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: nesterov\n",
            "ALL acc: [None, None, None, None, None, None, None, None, None, None]\n",
            "684/684 [==============================] - 25s 36ms/step - loss: 13.6999 - acc: 0.0146\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}